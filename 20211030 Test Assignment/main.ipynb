{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOP7UOoUXxBxC5tt5riEfj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snXXphE0eYrx","executionInfo":{"status":"ok","timestamp":1682091722602,"user_tz":-330,"elapsed":34983,"user":{"displayName":"R","userId":"03754586612706178236"}},"outputId":"39e30b0b-d3f8-44bb-b38d-0b290db5ee1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["import chardet\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJYDzEAIYsxJ","executionInfo":{"status":"ok","timestamp":1682091725999,"user_tz":-330,"elapsed":3406,"user":{"displayName":"R","userId":"03754586612706178236"}},"outputId":"86687e17-ca4f-4983-8d22-95b8451bc961"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/test_assignment/20211030 Test Assignment-20230419T154448Z-001/20211030 Test Assignment"],"metadata":{"id":"1QgG8iOHYwY9","executionInfo":{"status":"ok","timestamp":1682091726001,"user_tz":-330,"elapsed":21,"user":{"displayName":"R","userId":"03754586612706178236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9169960-5ac2-4318-b8d2-7edb24e1bdbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/test_assignment/20211030 Test Assignment-20230419T154448Z-001/20211030 Test Assignment\n"]}]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/test_assignment/20211030 Test Assignment-20230419T154448Z-001/20211030 Test Assignment'"],"metadata":{"id":"K5KCCcsLZPF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","path_to = {\n","    'auditor': '{}/StopWords/StopWords_Auditor.txt'.format(directory),\n","    'dates_number': '{}/StopWords/StopWords_DatesandNumbers.txt'.format(directory),\n","    'generic': '{}/StopWords/StopWords_Generic.txt'.format(directory),\n","    'generic_long': '{}/StopWords/StopWords_GenericLong.txt'.format(directory),\n","    'geographic':'{}/StopWords/StopWords_Geographic.txt'.format(directory),\n","    'names': '{}/StopWords/StopWords_Names.txt'.format(directory),\n","    'currencies': '{}/StopWords/StopWords_Currencies.txt'.format(directory),\n","    'negative': '{}/MasterDictionary/negative-words.txt'.format(directory),\n","    'positive': '{}/MasterDictionary/positive-words.txt'.format(directory),\n","    'input_data': '{}/Input.xlsx'.format(directory)\n","}\n"],"metadata":{"id":"ImtB0yu0ZZS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkFURQmtI8EM"},"outputs":[],"source":["# @title helper functions\n","def get(url):\n","    try:\n","        response = requests.get(url)\n","    except Exception as e:\n","        print(e)\n","\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    try:\n","        title = soup.title.text.strip()\n","        title = title.split('|')[0]\n","    except Exception as e:\n","        try:\n","            title = soup.find(\"h1\", {'class': \"tdb-title-text\"}).text.strip()\n","        except:\n","            None\n","\n","    text = \"\"\n","    content = soup.find('div', {'class': 'td-post-content'})\n","    if content is None:\n","        return None, None\n","\n","    for p in content.find_all('p'):\n","        for strong in p.find_all('strong'):\n","            strong.decompose()  # Remove <strong> tags and their contents\n","        text += p.get_text().strip()\n","\n","    return title, text\n","\n","def get_titles_texts():\n","    titles = []\n","    texts = []\n","    for url in df_content.URL:\n","        title, text = get(url)\n","        titles.append(title)\n","        texts.append(text)\n","    return titles, texts\n","\n","def get_stopwords_from(aud_path = path_to['auditor'], dn_path=path_to['dates_number'], genr_path=path_to['generic'], genrL_path=path_to['generic_long'],geo_path = path_to['geographic'], names_path = path_to['names'],currnc_path=path_to['currencies']):\n","    with open(aud_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(aud_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        auditor = [line.strip() for line in lines]\n","\n","    with open(dn_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(dn_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        date_numbers = [line.strip() for line in lines]\n","\n","    with open(genr_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(genr_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        generic = [line.strip() for line in lines]\n","\n","    with open(genrL_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(genrL_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        generic_long = [line.strip() for line in lines]\n","\n","    with open(geo_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(geo_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        geographic = [line.strip() for line in lines]\n","\n","    with open(names_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(names_path, \"r\", encoding=result['encoding']) as file:\n","        lines = file.readlines()\n","        names = [line.strip() for line in lines]\n","\n","    with open(currnc_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(currnc_path, 'r', encoding=result['encoding']) as f:\n","        lines = f.readlines()\n","        currencies = [line.strip() for line in lines]\n","\n","    currencies_modified = [x.split('|') for x in currencies]\n","    currencies_redundant = []\n","    for words in currencies_modified:\n","        for word in words:\n","            currencies_redundant.append(word.strip())\n","            \n","    txt_files_stopwords = set(auditor + currencies + geographic + names + generic+generic_long + date_numbers + currencies_redundant)\n","    \n","    stop_words = set(stopwords.words('english')).union(txt_files_stopwords)\n","    for words in stop_words:\n","        words = words.lower()\n","        \n","    return stop_words\n","\n","def filter_sentence(text,stop_words):\n","  if not isinstance(text, str):\n","    return ''\n","  sentence = text\n","  word_tokens = word_tokenize(sentence)\n","  filtered_sentence = \" \".join([w for w in word_tokens if w not in stop_words])\n","  return filtered_sentence\n","\n","def get_categorical_words(filename_pos = path_to['positive'], filename_nega = path_to['negative']):\n"," \n","    with open(filename_pos, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(filename_pos, \"r\") as file:\n","        lines = file.readlines()\n","        positive_words = [line.strip() for line in lines]\n","\n","    with open(filename_nega, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    with open(filename_nega, 'r', encoding=result['encoding']) as f:\n","        lines = f.readlines()\n","        negative_words = [line.strip() for line in lines]\n","        \n","    return positive_words, negative_words\n","\n","def get_score(filtered_sentence, positive_words, negative_words):\n","    if not isinstance(filtered_sentence, str):\n","        return ''\n","    negative_score = 0\n","    positive_score = 0\n","    temp = filtered_sentence.split(' ')\n","    for word in temp:\n","        if word in positive_words:\n","            positive_score += 1\n","        elif word in negative_words:\n","            negative_score += -1\n","    return positive_score, -1*negative_score\n","\n","def get_polarity_scores(df,col1 = 'POSITIVE SCORE', col2 = 'NEGATIVE SCORE'):\n","    polarity_scores = []\n","    for pos_score, neg_score in zip(df[col1], df[col2]):\n","        polarity_score = (pos_score - neg_score)/(pos_score + neg_score + 0.000001)\n","        polarity_scores.append(polarity_score)\n","    return polarity_scores\n","\n","def get_words_and_sentences(df, col1 = 'filtered_texts'):\n","    words = []\n","    sent = []\n","    avg_sent_length = []\n","\n","    for sentences in df[col1]:\n","        sent_count = word_count = 0 \n","        for w in sentences.split(\" \"):\n","            if w in ['.','?','!']:\n","                sent_count +=1\n","            elif w not in ['%', '(', ')', ',', '’', '“', '”']:\n","                word_count += 1\n","        words.append(word_count)\n","        sent.append(sent_count)\n","        avg_sent_length.append(word_count/(sent_count + 0.000001))\n","\n","    return avg_sent_length, words, sent\n","\n","def get_subjectivity_scores(df, col1='POSITIVE SCORE', col2='NEGATIVE SCORE', col3='WORD COUNT'):\n","    sub_scores = []\n","    for pos, neg, words in zip(df[col1],df[col2],df[col3]):\n","        sub_score = (pos + neg)/ (words + 0.000001)\n","        sub_scores.append(sub_score)\n","    return sub_scores\n","\n","def count_complex_words(paragraph):\n","    vowels = re.compile(r'[aeiouyAEIOUY]+')\n","    exceptions = set(['es', 'ed'])\n","    words = paragraph.split()\n","    complex_syllables = 0\n","    syllables = 0\n","    for word in words:\n","        word = word.rstrip('.,;:?!')\n","        num_vowels = len(vowels.findall(word))\n","\n","        if num_vowels > 0 and word[-2:] not in exceptions:\n","            syllables += 1   \n","        if num_vowels > 2 and word[-2:] not in exceptions:\n","            complex_syllables += 1\n","   \n","    return complex_syllables, syllables\n","\n","def count_personal_pronouns(text):\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    pattern = r\"\\b(\" + \"|\".join(personal_pronouns) + r\")\\b\"\n","    matches = re.findall(pattern, text, re.IGNORECASE)\n","    return len(matches)\n","\n","def get_average_words(df, col = 'filtered_texts', col2= 'WORD COUNT'):\n","    avg_words_length = []\n","    for sentences, words in zip(df[col], df[col2]):\n","        chars = sentences.replace(\" \", \"\").replace('.',\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n","        awl = chars.__len__()/words\n","        avg_words_length.append(awl)\n","    return avg_words_length\n","\n","def get_words_per_sentences(text):\n","    if not isinstance(text, str):\n","        return ''\n","    \n","    word_tokens = word_tokenize(text)\n","    s = \" \".join([w for w in word_tokens])\n","    sent_count = word_count = 0\n","    for w in s.split(\" \"):\n","        if w in ['.','?','!']:\n","            sent_count +=1\n","        elif w not in ['%', '(', ')', ',', '’', '“', '”']:\n","            word_count += 1\n","    return (word_count/(sent_count + 0.000001))\n","\n","def save_dataframe(df, path):\n","    df.to_excel('{}.xlsx'.format(path), index=False)\n","    df.to_csv('{}.csv'.format(path), index=False)   \n"]},{"cell_type":"code","source":["# @title analyse\n","def analysed_texts_df(df):\n","    \n","    positive_words, negative_words = get_categorical_words()\n","    stop_words = get_stopwords_from()\n","    \n","    df['filtered_texts'] =  df['texts'].apply(filter_sentence, stop_words = stop_words)\n","    df['POSITIVE SCORE'], df['NEGATIVE SCORE'] = zip(*df.filtered_texts.apply(get_score,positive_words = positive_words, negative_words= negative_words))\n","    df['POLARITY SCORE'] = get_polarity_scores(df)\n","    df['AVG SENTENCE LENGTH'], df['WORD COUNT'], df['total_sentences'] = get_words_and_sentences(df)\n","    df['SUBJECTIVITY SCORE'] = get_subjectivity_scores(df)\n","    df['AVG NUMBER OF WORDS PER SENTENCE'] = (df.texts).apply(get_words_per_sentences)\n","    df['COMPLEX WORD COUNT'], df['SYLLABLES'] = zip(*df.filtered_texts.apply(count_complex_words))\n","    df['PERCENTAGE OF COMPLEX WORDS'] = df.apply(lambda row: 100*row['COMPLEX WORD COUNT']/row['WORD COUNT'], axis = 1)\n","    df['FOG INDEX'] = df.apply(lambda row: 0.4*(row['AVG SENTENCE LENGTH']) + (row['PERCENTAGE OF COMPLEX WORDS']), axis=1)\n","    df['SYLLABLE PER WORD'] = df.apply(lambda row: row['SYLLABLES']/(row['WORD COUNT']+ 0.000001), axis=1)\n","    df['PERSONAL PRONOUNS'] = df.filtered_texts.apply(count_personal_pronouns)\n","    df['AVG WORD LENGTH'] = get_average_words(df)\n","\n","    col = 'AVG NUMBER OF WORDS PER SENTENCE'\n","    df.loc[df[col] == 1000000.0] = 'NaN'\n","    col = 'FOG INDEX'\n","    df.loc[df[col] == 4000000.0] = 'NaN'\n","\n","    new_order = ['URL_ID','URL','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE','AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT','SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n","    df = df.reindex(columns=new_order)\n","\n","    return df"],"metadata":{"id":"HFFSroJWaiIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title main\n","if __name__ == '__main__':\n","    \n","    df_content = pd.read_excel(path_to['input_data'])\n","    df_content['titles'], df_content['texts'] = get_titles_texts()\n","    \n","    # save \n","    save_loc = 'scrapped_data'\n","    save_dataframe(df_content, save_loc)\n","    \n","    \n","    df = pd.read_csv('{}/{}.csv'.format(directory,save_loc))\n","    \n","    df = analysed_texts_df(df)\n","        \n","    # save\n","    save_loc = 'OUTPUT DATA STRUCTURE'\n","    save_dataframe(df, save_loc)"],"metadata":{"id":"DQXNg6YBacFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"3j0taqTvdeiU","executionInfo":{"status":"ok","timestamp":1682091868017,"user_tz":-330,"elapsed":788,"user":{"displayName":"R","userId":"03754586612706178236"}},"colab":{"base_uri":"https://localhost:8080/","height":731},"outputId":"fc19337b-9d79-4669-90d9-3b95efc60ae8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    URL_ID                                                URL POSITIVE SCORE  \\\n","0       37  https://insights.blackcoffer.com/ai-in-healthc...             63   \n","1       38  https://insights.blackcoffer.com/what-if-the-c...             55   \n","2       39  https://insights.blackcoffer.com/what-jobs-wil...             64   \n","3       40  https://insights.blackcoffer.com/will-machine-...             54   \n","4       41  https://insights.blackcoffer.com/will-ai-repla...             47   \n","..     ...                                                ...            ...   \n","109    146  https://insights.blackcoffer.com/blockchain-fo...             21   \n","110    147  https://insights.blackcoffer.com/the-future-of...             34   \n","111    148  https://insights.blackcoffer.com/big-data-anal...             26   \n","112    149  https://insights.blackcoffer.com/business-anal...             28   \n","113    150  https://insights.blackcoffer.com/challenges-an...             32   \n","\n","    NEGATIVE SCORE POLARITY SCORE SUBJECTIVITY SCORE AVG SENTENCE LENGTH  \\\n","0               31       0.340426           0.093906               20.02   \n","1               36       0.208791           0.144674             9.38806   \n","2               34       0.306122            0.11086                13.6   \n","3               21           0.44           0.107296            9.197368   \n","4               21       0.382353           0.081146           15.236363   \n","..             ...            ...                ...                 ...   \n","109             27         -0.125           0.100629           12.230769   \n","110             11       0.511111           0.075251           15.736842   \n","111             38        -0.1875           0.107023               11.96   \n","112              3       0.806452           0.106897           18.124999   \n","113             37      -0.072464           0.133462            8.913793   \n","\n","    PERCENTAGE OF COMPLEX WORDS  FOG INDEX AVG NUMBER OF WORDS PER SENTENCE  \\\n","0                      40.45954   48.46754                        34.619999   \n","1                     28.616852  32.372076                         21.02985   \n","2                     40.045249  45.485249                        25.661538   \n","3                     31.473534  35.152481                         20.43421   \n","4                     36.038186  42.132732                        30.599999   \n","..                          ...        ...                              ...   \n","109                   34.591195  39.483503                        22.564102   \n","110                   34.280936  40.575673                        28.605262   \n","111                   38.294314  43.078314                             21.7   \n","112                   46.206897  53.456896                        34.499998   \n","113                   34.622824  38.188341                        16.965517   \n","\n","    COMPLEX WORD COUNT WORD COUNT SYLLABLE PER WORD PERSONAL PRONOUNS  \\\n","0                  405       1001          0.856144                 0   \n","1                  180        629          0.837838                 1   \n","2                  354        884          0.837104                 1   \n","3                  220        699          0.885551                 1   \n","4                  302        838          0.849642                 4   \n","..                 ...        ...               ...               ...   \n","109                165        477          0.865828                 6   \n","110                205        598          0.834448                 1   \n","111                229        598          0.897993                 0   \n","112                134        290          0.855172                 0   \n","113                179        517          0.880077                 1   \n","\n","    AVG WORD LENGTH  \n","0          7.747253  \n","1          7.063593  \n","2           7.56448  \n","3          6.815451  \n","4          7.167064  \n","..              ...  \n","109        7.436059  \n","110        6.991639  \n","111        6.966555  \n","112        7.968966  \n","113        6.945841  \n","\n","[114 rows x 15 columns]"],"text/html":["\n","  <div id=\"df-197756a1-3d05-43fa-bb2b-e35aebb0b82e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","      <th>POSITIVE SCORE</th>\n","      <th>NEGATIVE SCORE</th>\n","      <th>POLARITY SCORE</th>\n","      <th>SUBJECTIVITY SCORE</th>\n","      <th>AVG SENTENCE LENGTH</th>\n","      <th>PERCENTAGE OF COMPLEX WORDS</th>\n","      <th>FOG INDEX</th>\n","      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n","      <th>COMPLEX WORD COUNT</th>\n","      <th>WORD COUNT</th>\n","      <th>SYLLABLE PER WORD</th>\n","      <th>PERSONAL PRONOUNS</th>\n","      <th>AVG WORD LENGTH</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","      <td>63</td>\n","      <td>31</td>\n","      <td>0.340426</td>\n","      <td>0.093906</td>\n","      <td>20.02</td>\n","      <td>40.45954</td>\n","      <td>48.46754</td>\n","      <td>34.619999</td>\n","      <td>405</td>\n","      <td>1001</td>\n","      <td>0.856144</td>\n","      <td>0</td>\n","      <td>7.747253</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","      <td>55</td>\n","      <td>36</td>\n","      <td>0.208791</td>\n","      <td>0.144674</td>\n","      <td>9.38806</td>\n","      <td>28.616852</td>\n","      <td>32.372076</td>\n","      <td>21.02985</td>\n","      <td>180</td>\n","      <td>629</td>\n","      <td>0.837838</td>\n","      <td>1</td>\n","      <td>7.063593</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","      <td>64</td>\n","      <td>34</td>\n","      <td>0.306122</td>\n","      <td>0.11086</td>\n","      <td>13.6</td>\n","      <td>40.045249</td>\n","      <td>45.485249</td>\n","      <td>25.661538</td>\n","      <td>354</td>\n","      <td>884</td>\n","      <td>0.837104</td>\n","      <td>1</td>\n","      <td>7.56448</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>https://insights.blackcoffer.com/will-machine-...</td>\n","      <td>54</td>\n","      <td>21</td>\n","      <td>0.44</td>\n","      <td>0.107296</td>\n","      <td>9.197368</td>\n","      <td>31.473534</td>\n","      <td>35.152481</td>\n","      <td>20.43421</td>\n","      <td>220</td>\n","      <td>699</td>\n","      <td>0.885551</td>\n","      <td>1</td>\n","      <td>6.815451</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n","      <td>47</td>\n","      <td>21</td>\n","      <td>0.382353</td>\n","      <td>0.081146</td>\n","      <td>15.236363</td>\n","      <td>36.038186</td>\n","      <td>42.132732</td>\n","      <td>30.599999</td>\n","      <td>302</td>\n","      <td>838</td>\n","      <td>0.849642</td>\n","      <td>4</td>\n","      <td>7.167064</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>146</td>\n","      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>-0.125</td>\n","      <td>0.100629</td>\n","      <td>12.230769</td>\n","      <td>34.591195</td>\n","      <td>39.483503</td>\n","      <td>22.564102</td>\n","      <td>165</td>\n","      <td>477</td>\n","      <td>0.865828</td>\n","      <td>6</td>\n","      <td>7.436059</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>147</td>\n","      <td>https://insights.blackcoffer.com/the-future-of...</td>\n","      <td>34</td>\n","      <td>11</td>\n","      <td>0.511111</td>\n","      <td>0.075251</td>\n","      <td>15.736842</td>\n","      <td>34.280936</td>\n","      <td>40.575673</td>\n","      <td>28.605262</td>\n","      <td>205</td>\n","      <td>598</td>\n","      <td>0.834448</td>\n","      <td>1</td>\n","      <td>6.991639</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>148</td>\n","      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n","      <td>26</td>\n","      <td>38</td>\n","      <td>-0.1875</td>\n","      <td>0.107023</td>\n","      <td>11.96</td>\n","      <td>38.294314</td>\n","      <td>43.078314</td>\n","      <td>21.7</td>\n","      <td>229</td>\n","      <td>598</td>\n","      <td>0.897993</td>\n","      <td>0</td>\n","      <td>6.966555</td>\n","    </tr>\n","    <tr>\n","      <th>112</th>\n","      <td>149</td>\n","      <td>https://insights.blackcoffer.com/business-anal...</td>\n","      <td>28</td>\n","      <td>3</td>\n","      <td>0.806452</td>\n","      <td>0.106897</td>\n","      <td>18.124999</td>\n","      <td>46.206897</td>\n","      <td>53.456896</td>\n","      <td>34.499998</td>\n","      <td>134</td>\n","      <td>290</td>\n","      <td>0.855172</td>\n","      <td>0</td>\n","      <td>7.968966</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>150</td>\n","      <td>https://insights.blackcoffer.com/challenges-an...</td>\n","      <td>32</td>\n","      <td>37</td>\n","      <td>-0.072464</td>\n","      <td>0.133462</td>\n","      <td>8.913793</td>\n","      <td>34.622824</td>\n","      <td>38.188341</td>\n","      <td>16.965517</td>\n","      <td>179</td>\n","      <td>517</td>\n","      <td>0.880077</td>\n","      <td>1</td>\n","      <td>6.945841</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>114 rows × 15 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-197756a1-3d05-43fa-bb2b-e35aebb0b82e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-197756a1-3d05-43fa-bb2b-e35aebb0b82e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-197756a1-3d05-43fa-bb2b-e35aebb0b82e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]}]}